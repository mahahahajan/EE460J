Natasha: Faster Non-Convex Stochastic Optimization

via Strongly Non-Convex Parameter

Zeyuan Allen-Zhu 1

Abstract

Given a non-convex function f (x) that is an av-
erage of n smooth functions, we design stochas-
tic ï¬rst-order methods to ï¬nd its approximate
stationary points. The performance of our new
methods depend on the smallest (negative) eigen-
value âˆ’Ïƒ of the Hessian. This parameter Ïƒ
captures how strongly non-convex f (x) is, and
is analogous to the strong convexity parameter
for convex optimization. At least in theory, our
methods outperform known results for a range of
parameter Ïƒ, and can also be used to ï¬nd approx-
imate local minima. Our result implies an inter-
esting dichotomy: there exists a threshold Ïƒ0 so
that the (currently) fastest methods for Ïƒ > Ïƒ0
and for Ïƒ < Ïƒ0 have different behaviors: the for-
mer scales with n2/3 and the latter scales with
n3/4.

1 Introduction
We study the problem of composite non-convex minimiza-
tion:

(cid:110)

(cid:111)

n(cid:88)

i=1

1
n

fi(x)

F (x) := Ïˆ(x) + f (x) := Ïˆ(x) +

min
xâˆˆRd
(1.1)
where each fi(x) is nonconvex but smooth, and Ïˆ(Â·) is
proper convex, possibly nonsmooth, but relatively simple.
We are interested in ï¬nding a point x that is an approximate
local minimum of F (x).
â€¢ The ï¬nite-sum structure f (x) = 1

i=1 fi(x) arises
prominently in large-scale machine learning tasks. In
particular, when minimizing loss over a training set,
each example i corresponds to one loss function fi(Â·)
in the summation. This ï¬nite-sum structure allows one
to perform stochastic gradient descent with respect to a

(cid:80)n

n

Future version of this paper shall be found at http://
arxiv.org/abs/1702.00763. 1Microsoft Research. Cor-
respondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

random âˆ‡fi(x).

â€¢ The so-called proximal term Ïˆ(x) adds more general-
ity to the model. For instance, if Ïˆ(x) is the indicator
function of a convex set, then problem (1.1) becomes
constraint minimization; if Ïˆ(x) = (cid:107)x(cid:107)1, then we can
allow problem (1.1) to perform feature selection.
In
general, Ïˆ(x) has to be a simple function where the
2Î·(cid:107)x âˆ’ x0(cid:107)2}
projection operation arg minx{Ïˆ(x) + 1
is efï¬ciently computable. At a ï¬rst reading of this pa-
per, one can assume Ïˆ(x) â‰¡ 0 for simplicity.

Many non-convex machine learning problems fall into
problem (1.1). Most notably, training deep neural networks
and classiï¬cations with sigmoid loss correspond to (1.1)
where neither fi(x) or f (x) is convex. However, our un-
derstanding to this challenging non-convex problem is very
limited.
1.1 Strongly Non-Convex Optimization
Let L be the smoothness parameter for each fi(x), meaning
all the eigenvalues of âˆ‡2fi(x) lie in [âˆ’L, L].1
We denote by Ïƒ âˆˆ [0, L] the strong-nonconvexity parameter
of f (x) = 1
n

i=1 fi(x), meaning that

(cid:80)n

all the eigenvalues of âˆ‡2f (x) lie in [âˆ’Ïƒ, L].

We emphasize that parameter Ïƒ is analogous to the strong-
convexity parameter Âµ for convex optimization, where all
the eigenvalues of âˆ‡2f (x) lie in [Âµ, L] for some Âµ > 0.
We wish to ï¬nd an Îµ-approximate stationary point (a.k.a.
critical point) of F (x), that is

a point x satisfying (cid:107)G(x)(cid:107) â‰¤ Îµ

where G(x) is the so-called gradient mapping of F (x) (see
Section 2 for a formal deï¬nition).
In the special case of
Ïˆ(Â·) â‰¡ 0, gradient mapping G(x) is the same as gradient
âˆ‡f (x), so x satisï¬es (cid:107)âˆ‡f (x)(cid:107) â‰¤ Îµ.
Since f (Â·) is Ïƒ-strongly nonconvex, any Îµ-approximate sta-
tionary point is automatically also an (Îµ, Ïƒ)-approximate
local minimum â€” meaning that the Hessian of the output
point âˆ‡2f (x) (cid:23) âˆ’ÏƒI is approximately positive semideï¬-
nite (PSD).

1This deï¬nition also applies to functions f (x) that are not

twice differentiable, see Section 2 for details.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

1.2 Motivations and Remarks
â€¢ We focus on strongly non-convex optimization because
introducing this parameter Ïƒ allows us to perform a
more reï¬ned study of non-convex optimization.
If Ïƒ
equals L then L-strongly nonconvex optimization is
equivalent to the general non-convex optimization.

â€¢ We focus only on ï¬nding stationary points as op-
posed to local minima, because in a recent study â€”
see Appendix Aâ€” researchers have shown that ï¬nding
(Îµ, Î´)-approximate local minima reduces to ï¬nding Îµ-
approximate stationary points in an O(Î´)-strongly non-
convex function.

â€¢ Parameter Ïƒ is often not constant and can be much
smaller than L. For instance, second-order methods of-
Îµ)-approximate local minima (Nesterov,
ten ï¬nd (Îµ,
2008) and this corresponds to Ïƒ =

âˆš

âˆš

Îµ.

Figure 1: Comparison to prior works

function. Under mild assumption Ïƒ â‰¥ Îµ2, this approach
â€¢ ï¬nds an Îµ-approximate stationary point in gradient

complexity (cid:101)O(cid:0) nÏƒ+n3/4

(cid:1).

âˆš

LÏƒ

Îµ2

1.3 Known Results
Despite the widespread use of nonconvex models in ma-
chine learning and related ï¬elds, our understanding to non-
convex optimization is still very limited. Until recently,
nearly all research papers have been mostly focusing on ei-
ther Ïƒ = 0 or Ïƒ = L:
â€¢ If Ïƒ = 0,

the accelerated SVRG method (Shalev-
Shwartz, 2016; Allen-Zhu & Yuan, 2016) ï¬nds x sat-
isfying F (x) âˆ’ F (xâˆ—) â‰¤ Îµ, in gradient complexity

(cid:101)O(cid:0)n + n3/4(cid:112)L/Îµ(cid:1).2 This result is irrelevant to this

paper because f (x) is simply convex.

â€¢ If Ïƒ = L, the SVRG method (Allen-Zhu & Hazan,
2016) ï¬nds an Îµ-approximate stationary point of F (x)
in gradient complexity O(n + n2/3L/Îµ2).

â€¢ If Ïƒ = L, gradient descent ï¬nds an Îµ-approximate sta-

tionary point in gradient complexity O(nL/Îµ2).

â€¢ If Ïƒ = L, stochastic gradient descent ï¬nds an Îµ-approx.

stationary point in gradient complexity O(L2/Îµ4).

Throughout this paper, we refer to gradient complexity
as the total number of stochastic gradient computations
âˆ‡fi(x) and proximal computations y â† ProxÏˆ,Î·(x) :=
arg miny{Ïˆ(y) + 1
2Î·(cid:107)y âˆ’ x(cid:107)2}.3
Very recently,
it was observed by two independent
groups (Agarwal et al., 2017; Carmon et al., 2016) â€”
although implicitly, see Section 2.1â€” that for solving the
Ïƒ-strongly nonconvex problem, one can repeatedly regu-
larize F (x) to make it Ïƒ-strongly convex, and then apply
the accelerated SVRG method to minimize this regularized

2We use (cid:101)O to hide poly-logarithmic factors in n, L, 1/Îµ.

3Some authors also refer to them as incremental ï¬rst-order or-
acle (IFO) and proximal oracle (PO) calls. In most machine learn-
ing applications, each IFO and PO call can be implemented to run
in time O(d) where d is the dimension of the model, or even in
time O(s) if s is the average sparsity of the data vectors.

We call this method repeatSVRG in this paper. Unfortu-
nately, repeatSVRG is even slower than the vanilla SVRG
for Ïƒ = L by a factor n1/3, see Figure 1.
Remark on SGD. Stochastic gradient descent (SGD) has
a slower convergence rate (i.e., in terms of 1/Îµ4) than other
cited ï¬rst-order methods (i.e., in terms of 1/Îµ2), see for
instance (Ghadimi & Lan, 2015). However, the complexity
of SGD does not depend on n and thus is incomparable to
gradient descent, SVRG, or repeatSVRG.4 This is one of
the main motivations to study how to reduce the complexity
of non-SGD methods, especially in terms of n.
1.4 Our New Results
In this paper, we identify an interesting dichotomy with re-
spect to the spectrum of the nonconvexity parameter Ïƒ âˆˆ
[0, L]. In particular, we showed that if Ïƒ â‰¥ L/
n, then
our new method Natasha ï¬nds an Îµ-approximate station-
ary point of F (x) in gradient complexity
n2/3(L2Ïƒ)1/3

(cid:16)

(cid:17)

âˆš

O

n log

+

1
Îµ

Îµ2

.

In other words, together with repeatSVRG, we have im-
proved the gradient complexity for Ïƒ-stringly nonconvex
optimization to5

LÏƒ

n2/3(L2Ïƒ)1/3

,

âˆš
and the ï¬rst term in the min is smaller if Ïƒ < L/
the second term is smaller if Ïƒ > L/

n and
n. We illustrate our

Îµ2
âˆš

Îµ2

(cid:16)

(cid:101)O

min

(cid:110) n3/4

âˆš

(cid:111)(cid:17)

4In practice, there are examples in non-convex empirical risk
minimization (Allen-Zhu & Hazan, 2016) and in training neural
networks (Allen-Zhu & Hazan, 2016; Reddi et al., 2016) where
SVRG can outperform SGD. Of course, for deep learning tasks,
SGD remains to be the best practical method of choice.

5We remark here that this is under mild assumptions for Îµ be-
ing sufï¬ciently small. For instance, the result of (Agarwal et al.,
2017; Carmon et al., 2016) requires Îµ2 â‰¤ Ïƒ. In our result, the
term n log 1

Îµ disappears when Îµ6 â‰¤ L2Ïƒ/n.

complexity (log-scale)ğœ=ğ¿/ğ‘›ğœ=ğ¿repeatSVRGNatashaSVRGğœ=0ğ‘›2/3ğ¿2ğœ1/3ğœ€2ğ‘›3/4ğ¿ğœ1/2ğœ€2ğ‘›2/3ğ¿ğœ€2ğ‘›ğ¿ğœ€2gradient descentğ‘›ğœğœ€2Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Ïƒ2 â‰¤ n2.

performance improvement in Figure 1. Our result matches
that of SVRG for Ïƒ = L, and has a much simpler analysis.
Additional Results. One can take a step further and ask
what if each function fi(x) is ((cid:96)1, (cid:96)2)-smooth for parame-
ters (cid:96)1, (cid:96)2 â‰¥ Ïƒ, meaning that all the eigenvalues of âˆ‡2fi(x)
lie in [âˆ’(cid:96)2, (cid:96)1].
We show that a variant of our method, which we call
Natashafull, solves this more reï¬ned problem of (1.1) with

total gradient complexity O(cid:0)n log 1

(cid:1) as

Îµ + n2/3((cid:96)1(cid:96)2Ïƒ)1/3

long as (cid:96)1(cid:96)2
Remark 1.1. In applications, (cid:96)1 and (cid:96)2 can be of very dif-
ferent magnitudes. The most inï¬‚uential example is ï¬nding
the leading eigenvector of a symmetric matrix. Using the
so-called shift-and-invert reduction (Garber et al., 2016),
computing the leading eigenvector reduces to the con-
vex version of problem (1.1), where each fi(x) is (Î», 1)-
smooth for Î» (cid:28) 1. Other examples include all the ap-
plications that are built on shift-and-invert, including high
rank SVD/PCA (Allen-Zhu & Li, 2016), canonical compo-
nent analysis (Allen-Zhu & Li, 2017a), online matrix learn-
ing (Allen-Zhu & Li, 2017b), and approximate local min-
ima algorithms (Agarwal et al., 2017; Carmon et al., 2016).

Îµ2

Mini-Batch. Our result generalizes trivially to the mini-
batch stochastic setting, where in each iteration one com-
putes âˆ‡fi(x) for b random choices of index i âˆˆ [n] and av-
erage them. The stated gradient complexities of Natasha
and Natashafull can be adjusted so that the factor n2/3 is
replaced with n2/3b1/3.

1.5 Our Techniques
Let us ï¬rst recall the main idea behind stochastic variance-
reduced methods, such as SVRG (Johnson & Zhang, 2013).
The SVRG method divides iterations into epochs, each of

length n. It maintains a snapshot point(cid:101)x for each epoch,
and computes the full gradient âˆ‡f ((cid:101)x) only for snapshots.
estimator(cid:101)âˆ‡ = âˆ‡fi(xt)âˆ’âˆ‡fi((cid:101)x) +âˆ‡f ((cid:101)x) which satisï¬es
Ei[(cid:101)âˆ‡] = âˆ‡f (xt), and performs proximal update xt+1 â†
(cid:0)xt âˆ’ Î±(cid:101)âˆ‡(cid:1) for some learning rate Î±. (Recall that
if Ïˆ(Â·) â‰¡ 0 then we would have xt+1 â† xt âˆ’ Î±(cid:101)âˆ‡.)

Then, in each iteration t at point xt, SVRG deï¬nes gradient

ProxÏˆ,Î±

In nearly all
the aforementioned results for noncon-
vex optimization, researchers have either directly applied
SVRG (Allen-Zhu & Hazan, 2016) (for the case Ïƒ = L),
or repeatedly applied SVRG (Agarwal et al., 2017; Carmon
et al., 2016) (for general Ïƒ âˆˆ [0, L]). This puts some lim-
itation in the algorithmic design, because SVRG requires
each epoch to be of length exactly n.6

6The epoch length of SVRG is always n (or a constant mul-
tiple of n in practice), because this ensures the computation of

(cid:101)âˆ‡ is of amortized gradient complexity O(1). The per-iteration

complexity of SVRG is thus the same as the traditional stochastic

2 zt + 1

2 zt + 1

L2 n)1/3. Then, we

theory suggests the choice p â‰ˆ ( Ïƒ2

equivalent to replacing f (x) with its regularized version

same way as SVRG. However, we do not apply compute

â€¢ In our base algorithm Natasha, we divide each epoch

epochs. We provide pseudocode in Algorithm 1 and il-
lustrate it in Figure 2.

Our New Idea.
In this paper, we propose Natasha and
Natashafull, two methods that are no longer black-box re-
ductions to SVRG. Both of them still divide iterations into

epochs of length n, and compute gradient estimators (cid:101)âˆ‡ the
xt âˆ’ Î±(cid:101)âˆ‡ directly.
into p sub-epochs, each with a starting vector(cid:98)x. Our
replace the use of (cid:101)âˆ‡ with (cid:101)âˆ‡ + 2Ïƒ(xt âˆ’(cid:98)x). This is
f (x) + Ïƒ(cid:107)xâˆ’(cid:98)x(cid:107)2, where the center(cid:98)x varies across sub-
We view this additional term 2Ïƒ(xt âˆ’(cid:98)x) as a type of
the vector a bit in the backward direction towards(cid:98)x.
dates zt+1 â† ProxÏˆ,Î±(zt âˆ’ Î±(cid:101)âˆ‡) with respect to a dif-
2(cid:98)x
and compute gradient estimators (cid:101)âˆ‡ at points xt. We
2(cid:98)x as another type
ing towards(cid:98)x. The technique of computing gradients at

â€¢ In our full algorithm Natashafull, we add one more in-
gredient on top of Natasha. That is, we perform up-
ferent sequence {zt}, and then deï¬ne xt = 1

provide pseudocode in Algorithm 2 in the appendix.
We view this averaging xt = 1
of retraction, which stabilizes the algorithm by mov-

retraction, which stabilizes the algorithm by moving

points xt but moving a different sequence of points zt is
related to the Katyusha momentum recently developed
for convex optimization (Allen-Zhu, 2017).

1.6 Other Related Work
Methods based on variance-reduced stochastic gradients
were ï¬rst introduced for convex optimization. The ï¬rst
such method is SAG by Schmidt et al (Schmidt et al.,
2013). The two most popular choices for gradient estima-
tors are the SVRG-like one we adopted in this paper (inde-
pendently introduced by (Johnson & Zhang, 2013; Zhang
et al., 2013), and the SAGA-like one introduced by (De-
fazio et al., 2014).
In nearly all applications, the results
proven for SVRG-like estimators and SAGA-like estima-
tors are simply exchangeable (therefore, the results of this
paper naturally generalize to SAGA-like estimators).
The ï¬rst â€œnon-convex useâ€ of variance reduction is by
Shalev-Shwartz (Shalev-Shwartz, 2016) who assumes that
each fi(x) is non-convex but their average f (x) is still con-
vex. This result has been slightly improved to several more
reï¬ned settings (Allen-Zhu & Yuan, 2016). The ï¬rst truly
non-convex use of variance reduction (i.e., for f (x) being
also non-convex) is independently by (Allen-Zhu & Hazan,
2016) and (Reddi et al., 2016). First-order methods only

gradient descent (SGD).

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Figure 2: One full epoch of Natasha. The n iterations are divided into p sub-epochs, each consisting of m = n/p steps.

Îµ1.75

âˆš

ï¬nd stationary points (unless there is extra assumption on
the randomness of the data), and converge no faster than
1/Îµ2.
When the second-order Hessian information is used, one
can (1) ï¬nd local minima instead of stationary points, and
(2) improve the 1/Îµ2 rate to 1/Îµ1.5. The ï¬rst such re-
sult is by cubic-regularized Newtonâ€™s method (Nesterov,
2008); however, its per-iteration complexity is very high.
Very recently, two independent groups of authors tackled
this problem from a somewhat similar viewpoint (Carmon
et al., 2016; Agarwal et al., 2017): if the computation of
same order of the computation of gradients âˆ‡fi(x),7 then
Îµ)-approximate local minimum in
one can obtain a (Îµ,
Îµ1.5 + n3/4

Hessian-vector multiplications (i.e.,(cid:0)âˆ‡2fi(x)(cid:1)v) is on the
(cid:1), if we use big-O to
gradient complexity (cid:101)O(cid:0) n

also hide dependencies on the smoothness parameters.
Other related papers include Ge et al. (Ge et al., 2015)
where the authors showed that a noise-injected version of
SGD converges to local minima instead of critical points,
as long as the underlying function is â€œstrict-saddle.â€ Their
theoretical running time is a large polynomial in the dimen-
sion. Lee et al.
(Lee et al., 2016) showed that gradient
descent, starting from a random point, almost surely con-
verges to a local minimum if the function is â€œstrict-saddleâ€.
The rate of convergence required is somewhat unknown.
2 Preliminaries
Throughout this paper, we denote by (cid:107) Â· (cid:107) the Euclidean
norm. We use i âˆˆR [n] to denote that i is generated from
[n] = {1, 2, . . . , n} uniformly at random. We denote by
âˆ‡f (x) the full gradient of function f if it is differentiable,
and âˆ‚f (x) any subgradient if f is only Lipschitz continu-
ous at point x. We let xâˆ— be any minimizer of F (x).
Recall some deï¬nitions on strong convexity (SC), strongly
nonconvexity, and smoothness.
Deï¬nition 2.1. For a function f : Rd â†’ R,

7A lot of interesting problems satisfy this property, including

training neural nets.

â€¢ f is Ïƒ-strongly convex if âˆ€x, y âˆˆ Rd, it satisï¬es

f (y) â‰¥ f (x) + (cid:104)âˆ‚f (x), y âˆ’ x(cid:105) +

(cid:107)x âˆ’ y(cid:107)2 .
â€¢ f is Ïƒ-strongly nonconvex if âˆ€x, y âˆˆ Rd, it satisï¬es
(cid:107)x âˆ’ y(cid:107)2 .

f (y) â‰¥ f (x) + (cid:104)âˆ‚f (x), y âˆ’ x(cid:105) âˆ’ Ïƒ
2

â€¢ f is ((cid:96)1, (cid:96)2)-smooth if âˆ€x, y âˆˆ Rd, it satisï¬es

Ïƒ
2

f (x) + (cid:104)âˆ‡f (x), y âˆ’ x(cid:105) + (cid:96)1

2 (cid:107)x âˆ’ y(cid:107)2 â‰¥ f (y)

â‰¥ f (x) + (cid:104)âˆ‡f (x), y âˆ’ x(cid:105) âˆ’ (cid:96)2
2

(cid:107)x âˆ’ y(cid:107)2 .

â€¢ f is L-smooth if it is (L, L)-smooth.
The ((cid:96)1, (cid:96)2)-smoothness parameters were introduced
in (Allen-Zhu & Yuan, 2016) to tackle the convex setting
of problem (1.1). The notion of strong nonconvexity is
also known as â€œalmost convexity (Carmon et al., 2016)â€
or â€œlower smoothness (Allen-Zhu & Yuan, 2016).â€ We re-
frain from using the name â€œalmost convexityâ€ because it
coincides with several other non-equivalent deï¬nitions in
optimization literatures.
Deï¬nition 2.2. Given a parameter Î· > 0, the gradient
mapping of F (Â·) in (1.1) at point x is

(cid:0)x âˆ’ x(cid:48)(cid:1)

GÎ·(x) :=

(cid:8)Ïˆ(y) +(cid:104)âˆ‡f (x), y(cid:105) + 1

1
Î·

2Î·(cid:107)y âˆ’ x(cid:107)2(cid:9).

where x(cid:48) = arg miny
In particular, if Ïˆ(Â·) â‰¡ 0, then GÎ·(x) â‰¡ âˆ‡f (x).
The following theorem for the SVRG method can be found
for instance in (Allen-Zhu & Yuan, 2016), which is built on
top of the results (Shalev-Shwartz, 2016; Lin et al., 2015;
Frostig et al., 2015):
Theorem 2.3 (SVRG). Let G(y) := Ïˆ(y) + 1
i=1 gi(y)
n
be Ïƒ-strongly convex, then the SVRG method ï¬nds a point
y satisfying G(y) âˆ’ G(yâˆ—) â‰¤ Îµ

â€¢ with gradient complexity O(cid:0)(n + L2
â€¢ with gradient complexity O(cid:0)(n + (cid:96)1(cid:96)2

gi(Â·) is L-smooth (for L â‰¥ Ïƒ); or

gi(Â·) is ((cid:96)1, (cid:96)2)-smooth (for (cid:96)1, (cid:96)2 â‰¥ Ïƒ).

(cid:80)n
(cid:1), if each
(cid:1), if each

Îµ

Ïƒ2 ) log 1

Ïƒ2 ) log 1

Îµ

â€¦regularized by ğœğ‘¥âˆ’ ğ‘¥2â€¦regularized by ğœğ‘¥âˆ’ ğ‘¥2â€¦regularized by ğœğ‘¥âˆ’ ğ‘¥2â€¦ ğ’™next  ğ’™â€¦â€¦â€¦we

1

1

f (x) with its regularized version f s(x) := f (x) + Ïƒ(cid:107)x âˆ’

makes sure that when performing update xt â† xt+1, we

to be a random one from {x0, . . . , xmâˆ’1}.
Full Method.
In Natashafull (see full version), we also
divide each full epoch into p sub-epochs. In each sub-epoch

each sub-epoch s, we start with a point x0 =(cid:98)x, and replace
(cid:98)x(cid:107)2. Then, in each iteration t of the sub-epoch s, we
â€¢ compute gradient estimator (cid:101)âˆ‡ with respect to f s(xt),
(cid:8)Ïˆ(y) + (cid:104)(cid:101)âˆ‡, y(cid:105) +
2Î±(cid:107)y âˆ’ xt(cid:107)2(cid:9) with learning rate Î±.
â€¢ perform update xt+1 = arg miny
Effectively, the introduction of the regularizer Ïƒ(cid:107)x âˆ’(cid:98)x(cid:107)2
also move a bit towards point(cid:98)x (i.e., retraction by regular-
ization). Finally, when the sub-epoch is done, we deï¬ne(cid:98)x
s, we start with a point x0 = z0 =(cid:98)x and deï¬ne f s(x) :=
f (x) + Ïƒ(cid:107)x âˆ’(cid:98)x(cid:107)2. However, this time in each iteration t,
â€¢ compute gradient estimator (cid:101)âˆ‡ with respect to f s(xt),
(cid:8)Ïˆ(y) + (cid:104)(cid:101)âˆ‡, y(cid:105) +
2Î±(cid:107)y âˆ’ zt(cid:107)2(cid:9) with learning rate Î±, and
â€¢ perform update zt+1 = arg miny
Effectively, the regularizer Ïƒ(cid:107)xâˆ’(cid:98)x(cid:107)2 makes sure that when
performing updates, we move a bit towards point(cid:98)x (i.e.,
2(cid:98)x also helps us move towards point(cid:98)x
Finally, when the sub-epoch is over, we deï¬ne(cid:98)x to be a

retraction by regularization); at the same time, the choice
xt+1 = 1
(i.e., retraction by the so-called â€œKatyusha momentumâ€9).
random one from the set {x0, . . . , xmâˆ’1}, and move to the
next sub-epoch.
4 A Sufï¬cient Stopping Criterion
In this section, we present a sufï¬cient condition for ï¬nding
approximate stationary points in a Ïƒ-strongly nonconvex
function. Lemma 4.1 below states that, if we regularize the

approximate saddle-point for F (x).

original function and deï¬ne G(x) := F (x) + Ïƒ(cid:107)x âˆ’(cid:98)x(cid:107)2
for an arbitrary point(cid:98)x, then the minimizer of G(x) is an
Lemma 4.1. Suppose G(y) = F (y) + Ïƒ(cid:107)y âˆ’(cid:98)x(cid:107)2 for some
given point(cid:98)x, and let xâˆ— be the minimizer of G(y). If we
(cid:3) we have the gradient
then for every Î· âˆˆ (cid:0)0,

minimize G(y) and obtain a point x satisfying

G(x) âˆ’ G(xâˆ—) â‰¤ Î´2Ïƒ ,

â€¢ choose xt+1 = 1

2 zt+1 + 1

2 zt+1 + 1

2(cid:98)x.

(cid:101)O(cid:0)n + n3/4(cid:112)L/Ïƒ(cid:1) and (cid:101)O(cid:0)n + n3/4((cid:96)1(cid:96)2Ïƒ2)1/4(cid:1).

If one performs acceleration, the running times become

2.1 RepeatSVRG
We recall the idea behind a simple algorithm â€”that we call
repeatSVRGâ€” which ï¬nds the Îµ-approximate stationary
points for problem (1.1) when f (x) is Ïƒ-strongly noncon-
vex. The algorithm is divided into stages. In each stage t,
consider a modiï¬ed function Ft(x) := F (x) + Ïƒ(cid:107)xâˆ’ xt(cid:107)2.
It is easy to see that Ft(x) is Ïƒ-strongly convex, so one can
apply the accelerated SVRG method to minimize Ft(x).
Let xt+1 be any sufï¬ciently accurate approximate mini-
mizer of Ft(x).8
Section 4) that xt+1 is an
Now, one can prove (c.f.
O(Ïƒ(cid:107)xt âˆ’ xt+1(cid:107))-approximate stationary point for F (x).
Therefore, if Ïƒ(cid:107)xt âˆ’ xt+1(cid:107) â‰¤ Îµ we can stop the algorithm
because we have already found an O(Îµ)-approximate sta-
If Ïƒ(cid:107)xt âˆ’ xt+1(cid:107) > Îµ , then it must sat-
tionary point.
isfy that F (xt) âˆ’ F (xt+1) â‰¥ Ïƒ(cid:107)xt âˆ’ xt+1(cid:107)2 â‰¥ â„¦(Îµ2/Ïƒ),
Îµ2 (F (x0) âˆ’
F âˆ—) stages. Therefore, the total gradient complexity is
T multiplied with the complexity of accelerated SVRG

but this cannot happen for more than T = O(cid:0) Ïƒ
in each stage (which is (cid:101)O(n + n3/4(cid:112)L/Ïƒ) according to

Theorem 2.3).
Remark 2.4. The complexity of repeatSVRG can be in-
ferred from (Agarwal et al., 2017; Carmon et al., 2016), but
is not explicitly stated. For instance, the paper (Carmon
et al., 2016) does not allow F (x) to have a non-smooth
proximal term Ïˆ(x), and applies accelerated gradient de-
scent instead of accelerated SVRG.

3 Our Algorithms
We introduce two variants of our algorithms: (1) the base
method Natasha targets on the simple regime when f (x)
and each fi(x) are both L-smooth, and (2) the full method
Natashafull targets on the more reï¬ned regime when f (x)
is L-smooth but each fi(x) is ((cid:96)1, (cid:96)2)-smooth.
Both methods follow the general idea of variance-reduced
in each inner-most iteration,
stochastic gradient descent:

they compute a gradient estimator (cid:101)âˆ‡ that is of the form
(cid:101)âˆ‡ = âˆ‡f ((cid:101)x)âˆ’âˆ‡fi((cid:101)x)+âˆ‡fi(x) and satisï¬es EiâˆˆR[n][(cid:101)âˆ‡] =
âˆ‡f (x). Here, (cid:101)x is a snapshot point that is changed once
for computing (cid:101)âˆ‡ is O(1) per-iteration.

every n iterations (i.e., for each different k = 1, 2, . . . , T (cid:48)
in the pseudocode), and we call it a full epoch for every
distinct k. Notice that the amortized gradient complexity

Base Method.
In Natasha (see Algorithm 1), as illus-
trated by Figure 2, we divide each full epoch into p sub-
epochs s = 0, 1, . . . , p âˆ’ 1, each of length m = n/p. In
8Since the accelerated SVRG method has a linear convergence
rate for strongly convex functions, the complexity to ï¬nd such
xt+1 only depends logarithmically on this accuracy.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

mapping

1

max{L,4Ïƒ}

(cid:107)GÎ·(x)(cid:107)2 â‰¤ 12Ïƒ2(cid:107)xâˆ— âˆ’(cid:98)x(cid:107)2 + O(cid:0)Î´2(cid:1) .

Notice that when Ïˆ(x) â‰¡ 0 this lemma is trivial, and can
be found for instance in (Carmon et al., 2016). The main

9The idea for this second kind of retraction, and the idea of
having the updates on a sequence zt but computing gradients at
points xt, is largely motivated by our recent work on the Katyusha
momentum and the Katyusha acceleration (Allen-Zhu, 2017).

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

, p, T (cid:48), Î±)

âˆ…
âˆ…, sub-epoch count p âˆˆ [n], epoch count T (cid:48), learning rate Î± > 0.

âˆ…; m â† n/p; X â† [];

1: (cid:98)x â† x

Algorithm 1 Natasha(x
Input: starting vector x
Output: vector xout.
2: for k â† 1 to T (cid:48) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for

for s â† 0 to p âˆ’ 1 do

for t â† 0 to m âˆ’ 1 do

(cid:101)x â†(cid:98)x; Âµ â† âˆ‡f ((cid:101)x);
x0 â†(cid:98)x; X â† [X,(cid:98)x];
(cid:101)âˆ‡ â† âˆ‡fi(xt) âˆ’ âˆ‡fi((cid:101)x) + Âµ + 2Ïƒ(xt âˆ’(cid:98)x)
(cid:98)x â† a random choice from {x0, x1, . . . , xmâˆ’1};

i â† a random choice from {1,Â·Â·Â· , n}.

(cid:8)Ïˆ(y) + 1

xt+1 = arg minyâˆˆRd

end for

end for

2Î±(cid:107)y âˆ’ xt(cid:107)2 + (cid:104)(cid:101)âˆ‡, y(cid:105)(cid:9)

14: (cid:98)x â† a random vector in X;
15: xout â† an approximate minimizer of G(y) := F (y) + Ïƒ(cid:107)y âˆ’(cid:98)x(cid:107)2 using SVRG.

16: return xout.

(cid:5) T (cid:48) full epochs
(cid:5) p sub-epochs in each epoch
(cid:5) m iterations in each sub-epoch

(cid:5) Ei[(cid:101)âˆ‡] = âˆ‡(cid:0)f (x) + Ïƒ(cid:107)x âˆ’(cid:98)x(cid:107)2(cid:1)(cid:12)(cid:12)xt

(cid:5) for practitioners, choose the average

(cid:5) for practitioners, choose the last

(cid:5) it sufï¬ces to run SVRG for O(n log 1

Îµ ) iterations.

technical difï¬culty arises in order to deal with Ïˆ(x) (cid:54)= 0.
The proof is included in the full version.
5 Base Method: Analysis for One Full Epoch
In this section, we consider problem (1.1) where each fi(x)
is L-smooth and F (x) is Ïƒ-strongly nonconvex. We use
our base method Natasha to minimize F (x), and analyze
its behavior for one full epoch in this section. We assume
Ïƒ â‰¤ L without loss of generality, because any L-smooth
function is also L-strongly nonconvex.
Notations. We introduce the following notations for anal-
ysis purpose only.

â€¢ Let(cid:98)xs be the vector(cid:98)x at the beginning of sub-epoch s.
t be the vector xt in sub-epoch s.
â€¢ Let f s(x) := f (x) + Ïƒ(cid:107)x âˆ’(cid:98)xs(cid:107)2, F s(x) := F (x) +
t be the index i âˆˆ [n] in sub-epoch s at iteration t.
Ïƒ(cid:107)x âˆ’(cid:98)xs(cid:107)2, and xsâˆ— := arg minx{F s(x)}.
â€¢ Let(cid:101)âˆ‡f s(xs
t )âˆ’âˆ‡fi((cid:101)x)+âˆ‡f ((cid:101)x)+2Ïƒ(xtâˆ’
(cid:98)x) where i = is
â€¢ Let (cid:101)âˆ‡f (xs
t ) âˆ’ âˆ‡fi((cid:101)x) + âˆ‡f ((cid:101)x) where

t ) := âˆ‡fi(xs
t ) := âˆ‡fi(xs

â€¢ Let xs
â€¢ Let is

t .

t .
i = is

We obviously have that f s(x) and F s(x) are Ïƒ-strongly
convex, and f s(x) is (L + 2Ïƒ)-smooth.
5.1 Variance Upper Bound
The following lemma gives an upper bound on the variance

of the gradient estimator (cid:101)âˆ‡f s(xs
(cid:2)(cid:107)(cid:101)âˆ‡f s(xs
t âˆ’(cid:98)xs(cid:107)2 + pL2(cid:80)sâˆ’1
k=0 (cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2 .

Lemma 5.1. We have Eis
pL2(cid:107)xs

t )(cid:107)2(cid:3) â‰¤

t ) âˆ’ âˆ‡f s(xs

t ):

t

t

t

k=0

Eis

EiâˆˆR[n]

Proof. We have

= EiâˆˆR[n]
xâ‰¤ EiâˆˆR[n]
yâ‰¤ pEiâˆˆR[n]

t ) âˆ’ âˆ‡f (xs

t ) âˆ’ âˆ‡f s(xs

(cid:2)(cid:107)(cid:101)âˆ‡f (xs
t )(cid:107)2(cid:3) = Eis
t )(cid:107)2(cid:3)
(cid:2)(cid:107)(cid:101)âˆ‡f s(xs
t ) âˆ’ âˆ‡f ((cid:101)x))(cid:1)(cid:13)(cid:13)2(cid:3)
(cid:2)(cid:13)(cid:13)(cid:0)âˆ‡fi(xs
t ) âˆ’ âˆ‡fi((cid:101)x)(cid:1) âˆ’(cid:0)âˆ‡f (xs
t ) âˆ’ âˆ‡fi((cid:101)x)(cid:13)(cid:13)2(cid:3)
(cid:2)(cid:13)(cid:13)âˆ‡fi(xs
t ) âˆ’ âˆ‡fi((cid:98)xs)(cid:13)(cid:13)2(cid:3)
(cid:2)(cid:13)(cid:13)âˆ‡fi(xs
(cid:2)(cid:13)(cid:13)âˆ‡fi((cid:98)xk) âˆ’ âˆ‡fi((cid:98)xk+1)(cid:13)(cid:13)2(cid:3)
+ p(cid:80)sâˆ’1
t âˆ’(cid:98)xs(cid:107)2 + pL2(cid:80)sâˆ’1
k=0 (cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2 .

zâ‰¤ pL2(cid:107)xs
Above, inequality x is because for any random vector Î¶ âˆˆ
y is because(cid:98)x0 =(cid:101)x and for any p vectors a1, a2, . . . , ap âˆˆ
Rd, it holds that E(cid:107)Î¶ âˆ’EÎ¶(cid:107)2 = E(cid:107)Î¶(cid:107)2âˆ’(cid:107)EÎ¶(cid:107)2; inequality
Rd, it holds that (cid:107)a1 +Â·Â·Â·+ap(cid:107)2 â‰¤ p(cid:107)a1(cid:107)2 +Â·Â·Â·+p(cid:107)ap(cid:107)2;
and inequality z is because each fi(Â·) is L-smooth. (cid:3)
5.2 Analysis for One Sub-Epoch
The following inequality is classically known as the â€œre-
gret inequalityâ€ for mirror descent (Allen-Zhu & Orecchia,
2017), and its proof is classical (see full version):
Fact 5.2.
2Î± âˆ’ (cid:107)xs
tâˆ’u(cid:107)2
(cid:107)xs

t+1 âˆ’ u(cid:105) + Ïˆ(xs
âˆ’ (cid:107)xs

(cid:104)(cid:101)âˆ‡f s(xs

t+1) âˆ’ Ïˆ(u) â‰¤

for every u âˆˆ Rd.

t+1âˆ’u(cid:107)2
2Î±

t+1âˆ’xs

t ), xs

t(cid:107)2

2Î±

The following lemma is our main contribution for the base
method Natasha.
Lemma 5.3. As long as Î± â‰¤ 1

2L+4Ïƒ , we have

E(cid:104)(cid:0)F s((cid:98)xs+1) âˆ’ F s(xsâˆ—)(cid:1)(cid:105)
â‰¤ E(cid:104) F s((cid:98)xs) âˆ’ F s(xsâˆ—)

+Î±pL2(cid:16) s(cid:88)

(cid:107)(cid:98)xkâˆ’(cid:98)xk+1(cid:107)2(cid:17)(cid:105)

.

ÏƒÎ±m/2

k=0

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

F s(xs
xâ‰¤ f s(xs

Proof. We ï¬rst compute that
t+1) âˆ’ F s(u) = f s(xs
t ) + (cid:104)âˆ‡f s(xs
âˆ’ f s(u) + Ïˆ(xs

t+1 âˆ’ xs
t ), xs
t+1) âˆ’ Ïˆ(u)

t+1) âˆ’ f s(u) + Ïˆ(xs
(cid:107)xs

L + 2Ïƒ

t+1) âˆ’ Ïˆ(u)
t âˆ’ xs

t+1(cid:107)2

t(cid:105) +

2

(cid:107)xs

t âˆ’ xs

t+1(cid:107)2

yâ‰¤ (cid:104)âˆ‡f s(xs

t ), xs
+ (cid:104)âˆ‡f s(xs

t(cid:105) +

t+1 âˆ’ xs
t ), xs

L + 2Ïƒ
2
t âˆ’ u(cid:105) + Ïˆ(xs

t+1) âˆ’ Ïˆ(u) .

(5.1)
Above, inequality x uses the fact that f s(Â·) is (L + 2Ïƒ)-
smooth; and inequality y uses the convexity of f s(Â·). Now,
we take expectation with respect to is
t on both sides of (5.1),
and derive that:

t+1)(cid:3) âˆ’ F s(u)

t

Eis
xâ‰¤ Eis

+
yâ‰¤ Eis

zâ‰¤ Eis

{â‰¤ Eis

t

t

2

L + 2Ïƒ

(cid:2)F s(xs
(cid:104)(cid:104)(cid:101)âˆ‡f s(xs
(cid:104)(cid:104)(cid:101)âˆ‡f s(xs
âˆ’ (cid:107)xs
(cid:104)
Î±(cid:13)(cid:13)(cid:101)âˆ‡f s(xs
(cid:104)

2Î±

t

t

Î±pL2(cid:107)xs
t âˆ’ u(cid:107)2
2Î±

(cid:107)xs

+

t+1 âˆ’ u(cid:107)2

t ) âˆ’ âˆ‡f s(xs
(cid:107)xs
t ) âˆ’ âˆ‡f s(xs

t âˆ’ xs

t ), xs

t âˆ’ xs
t+1(cid:107)2 + Ïˆ(xs
t âˆ’ xs
t ), xs
âˆ’ L + 2Ïƒ
(cid:107)xs

2

âˆ’(cid:0) 1

2Î±

t ) âˆ’ âˆ‡f s(xs

t )(cid:13)(cid:13)2 +
sâˆ’1(cid:88)
t âˆ’(cid:98)xs(cid:107)2 + Î±pL2
(cid:105)
âˆ’ (cid:107)xs
[(cid:101)âˆ‡f s(xs

t+1 âˆ’ u(cid:107)2

2Î±

k=0

t+1(cid:105) + (cid:104)(cid:101)âˆ‡f s(xs
(cid:105)

t+1) âˆ’ Ïˆ(u)
(cid:107)xs
t+1(cid:105) +

(cid:1)(cid:107)xs

t âˆ’ u(cid:107)2
2Î±
t+1 âˆ’ xs

t(cid:107)2(cid:105)
âˆ’ (cid:107)xs
(cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2

t âˆ’ u(cid:107)2
2Î±

.

(5.2)

t

t

t

2L+4Ïƒ

t ), xs

tâˆ’xs

t+1âˆ’u(cid:105)(cid:3) ;
2(cid:107)b(cid:107)2;

2(cid:107)a(cid:107)2 + 1

(cid:0)F s(xs

t ) implies
t ), xs

t+1 âˆ’ xs
t ), xs
t )âˆ’âˆ‡f s(xs

(cid:2)(cid:104)âˆ‡f s(xs
(cid:2)(cid:104)(cid:101)âˆ‡f s(xs

t )] = âˆ‡f s(xs
t(cid:105) + (cid:104)âˆ‡f s(xs
t ), xs

Above, inequality x is follows from (5.1) together with
the fact that Eis

t âˆ’ u(cid:105)(cid:3)
t+1(cid:105)+(cid:104)(cid:101)âˆ‡f s(xs
Eis
= Eis
inequality y uses Fact 5.2; inequality z uses Î± â‰¤ 1
together with Youngâ€™s inequality (cid:104)a, b(cid:105) â‰¤ 1
and inequality { uses Lemma 5.1.
Finally, choosing u = xsâˆ— to be the (unique) minimizer of
F s(Â·) = f s(Â·) + Ïˆ(Â·), and telescoping inequality (5.2) for
t = 0, 1, . . . , m âˆ’ 1, we have
t ) âˆ’ F s(xsâˆ—)(cid:1)(cid:105)
(cid:16)
mâˆ’1(cid:88)

t âˆ’(cid:98)xs(cid:107)2
(cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2(cid:17)(cid:105)
sâˆ’1(cid:88)
(cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2(cid:17)(cid:105)
â‰¤ E(cid:104) F s((cid:98)xs) âˆ’ F s(xsâˆ—)
+ Î±pmL2(cid:16) s(cid:88)
Above, the second inequality uses the fact that(cid:98)xs+1 is cho-
mâˆ’1} uniformly at random, as well as

E(cid:104) mâˆ’1(cid:88)
â‰¤ E(cid:104)(cid:107)xs

sen from {xs
the Ïƒ-strong convexity of F s(Â·).

0 âˆ’ xsâˆ—(cid:107)2
2Î±

Î±pL2(cid:107)xs

0, . . . , xs

+ Î±pL2

ÏƒÎ±

k=0

k=0

t=1

t=0

+

.

t ), xs

t+1 âˆ’ u(cid:105)

(cid:105)

t+1 âˆ’ u(cid:107)2

2Î±

1

Dividing both sides by m and rearranging the terms (using
E(cid:104)(cid:0)F s((cid:98)xs+1) âˆ’ F s(xsâˆ—)(cid:1)(cid:105)
2ÏƒÎ± â‰¥ 1), we have
â‰¤ E(cid:104) F s((cid:98)xs) âˆ’ F s(xsâˆ—)

+ Î±pL2(cid:16) s(cid:88)

(cid:107)(cid:98)xk âˆ’(cid:98)xk+1(cid:107)2(cid:17)(cid:105)

. (cid:3)

ÏƒÎ±m/2

k=0

5.3 Analysis for One Full Epoch
One can telescope Lemma 5.3 for an entire epoch and ar-
rive at the following lemma (see full version):
Lemma 5.4.
have

2L+4Ïƒ , Î± â‰¥ 4

If Î± â‰¤ 1

p2L2 , we

E(cid:104)(cid:0)F s((cid:98)xs) âˆ’ F s(xsâˆ—)(cid:1)(cid:105) â‰¤ 2E(cid:104)

Ïƒm and Î± â‰¤ Ïƒ
(cid:105)
F ((cid:98)x0) âˆ’ F ((cid:98)xp)

.

pâˆ’1(cid:88)

s=0

6 Base Method: Final Theorem
We are now ready to state and prove our main convergence
theorem for Natasha:
Theorem 1. Suppose in (1.1), each fi(x) is L-smooth
and F (x) is Ïƒ-strongly nonconvex for Ïƒ â‰¤ L. Then, if
p2L2 ), our base

L2 n)1/3(cid:1) and Î± = Î˜( Ïƒ
(cid:16) (L2Ïƒ)1/3n2/3
(cid:17) Â· (F (x

L2
method Natasha outputs a point xout satisfying
E[(cid:107)GÎ·(xout)(cid:107)2] â‰¤ O

Ïƒ2 â‰¤ n, p = Î˜(cid:0)( Ïƒ2
for every Î· âˆˆ (cid:0)0,

(cid:3). In other words, to obtain

) âˆ’ F âˆ—) .

T (cid:48)n

âˆ…

1

max{L,4Ïƒ}

E[(cid:107)GÎ·(xout)(cid:107)2] â‰¤ Îµ2, we need gradient complexity
) âˆ’ F âˆ—)

(L2Ïƒ)1/3n2/3

Â· (F (x

n log

+

O

âˆ…

(cid:16)

(cid:17)

.

1
Îµ

Îµ2

In the above theorem, we have assumed Ïƒ â‰¤ L without
loss of generality because any L-smooth function is also
Ïƒ2 â‰¤ n
L-strongly nonconvex. Also, we have assumed L2
and if this inequality does not hold, then one should apply
repeatSVRG for a faster running time (see Figure 1).

24L2 n(cid:1)1/3, m =
Proof of Theorem 1. We choose p = (cid:0) Ïƒ2
(cid:98)xp of the previous epoch equals(cid:98)x0 of the next epoch, we

n/p, and Î± = 4
2L+4Ïƒ , so we can apply
Lemma 5.4. If we telescope Lemma 5.4 for the entire al-
gorithm (which has T (cid:48) full epochs), and use the fact that

conclude that if we choose a random epoch and a random
subepoch s, we will have

6p2L2 â‰¤

Ïƒm = Ïƒ

1

E[F s((cid:98)xs) âˆ’ F s(xsâˆ—)] â‰¤ 2

âˆ…

) âˆ’ F âˆ—) .

pT (cid:48) (F (x

By the Ïƒ-strong convexity of F s(Â·), we have E[Ïƒ(cid:107)(cid:98)xs âˆ’
xsâˆ—(cid:107)2] â‰¤ 4
Now, F s(x) = F (x) + Ïƒ(cid:107)xâˆ’(cid:98)xs(cid:107)2 satisï¬es the assumption

of G(x) in Lemma 4.1. If we use the SVRG method (see
Theorem 2.3) to minimize the convex function F s(x), we

âˆ…
pT (cid:48) (F (x

) âˆ’ F âˆ—).

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

get an output xout satisfying F s(xout) âˆ’ F s(xsâˆ—) â‰¤ Îµ2Ïƒ in

We can therefore apply Lemma 4.1 and conclude that this
output xout satisï¬es

gradient complexity O(cid:0)(n + L2
(cid:17) Â· (F (x
(cid:16) Ïƒ
(cid:16) (L2Ïƒ)1/3n2/3

E[(cid:107)GÎ·(xout)(cid:107)2] â‰¤ O

pT (cid:48)

Ïƒ2 ) log 1

Îµ

= O

T (cid:48)n

(cid:1) â‰¤ O(n log 1

Îµ ).

âˆ…

(cid:17) Â· (F (x

) âˆ’ F âˆ—)
âˆ…

) âˆ’ F âˆ—) .

(cid:17)

In other words, we obtain E[(cid:107)GÎ·(xout)(cid:107)2] â‰¤ Îµ2 using

T (cid:48)n = O

n + (L2Ïƒ)1/3n2/3

Îµ2

Â· (F (x

âˆ…

) âˆ’ F âˆ—)

(cid:16)

computations of the stochastic gradients. Here, the additive
term n is because T (cid:48) â‰¥ 1.
Îµ ), the gradient complex-
Finally, adding this with O(n log 1
ity for the application of SVRG in the last line of Natasha,
we ï¬nish the proof of the total gradient complexity. (cid:3)

7 Full Method: Final Theorem
We analyze and state the main theorems for our full method
Natashafull in the full version of this paper.
8 Conclusion
Stochastic gradient descent and gradient descent (including
alternating minimization) have become the canonical meth-
ods for solving non-convex machine learning tasks. How-
ever, can we design new non-convex methods to run even
faster than SGD or GD?
This present paper tries to tackle this general question, by
providing a new Natasha method which is intrinsically dif-
ferent from GD or SGD. It runs faster than GD and SVRG-
based methods at least in theory. We hope that this could
be a non-negligible step towards our better understanding
of non-convex optimization.
Finally, our results give rise to an interesting dichotomy in
âˆš
the best-known complexity of ï¬rst-order non-convex opti-
âˆš
mization: the complexity scales with n3/4 for Ïƒ < L/
n
n. It remains open to investi-
and with n2/3 for Ïƒ > L/
gate whether this dichotomy is intrinsic, or we can design
a more efï¬cient algorithm that outperforms both.
References
Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding Approximate Lo-
cal Minima for Nonconvex Optimization in Linear Time.
In STOC, 2017.

Allen-Zhu, Zeyuan. Katyusha: The First Direct Accelera-

tion of Stochastic Gradient Methods. In STOC, 2017.

Allen-Zhu, Zeyuan and Hazan, Elad. Variance Reduction

for Faster Non-Convex Optimization. In NIPS, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecom-
position. In Proceedings of the 34th International Con-
ference on Machine Learning, ICML â€™17, 2017a.

Allen-Zhu, Zeyuan and Li, Yuanzhi. Follow the Com-
pressed Leader: Faster Online Learning of Eigenvectors
In Proceedings of the 34th Inter-
and Faster MMWU.
national Conference on Machine Learning, ICML â€™17,
2017b.

Allen-Zhu, Zeyuan and Orecchia, Lorenzo. Linear Cou-
pling: An Ultimate Uniï¬cation of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theo-
retical Computer Science, ITCS â€™17, 2017.

Allen-Zhu, Zeyuan and Yuan, Yang.

Improved SVRG
for Non-Strongly-Convex or Sum-of-Non-Convex Ob-
jectives. In ICML, 2016.

Carmon, Yair, Duchi, John C., Hinder, Oliver, and Sidford,
Aaron. Accelerated Methods for Non-Convex Optimiza-
tion. ArXiv e-prints, abs/1611.00756, November 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
SAGA: A Fast Incremental Gradient Method With Sup-
port for Non-Strongly Convex Composite Objectives.
In NIPS, 2014. URL http://arxiv.org/abs/
1407.0202.

Frostig, Roy, Ge, Rong, Kakade, Sham M., and Sidford,
Aaron. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk min-
imization. In ICML, volume 37, pp. 1â€“28, 2015. URL
http://arxiv.org/abs/1506.07512.

Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Robust shift-and-invert preconditioning: Faster
and more sample efï¬cient algorithms for eigenvector
computation. In ICML, 2016.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Es-
caping from saddle pointsâ€”online stochastic gradient
for tensor decomposition. In Proceedings of the 28th An-
nual Conference on Learning Theory, COLT 2015, 2015.

for

nonconvex

gradient methods

Ghadimi, Saeed and Lan, Guanghui.

and stochastic programming.

Acceler-
nonlin-
ated
Mathemati-
ear
cal Programming, pp. 1â€“26,
ISSN
10.1007/s10107-015-0871-8.
doi:
0025-5610.
URL
http://arxiv.org/abs/1310.
3787http://link.springer.com/10.1007/
s10107-015-0871-8.

feb 2015.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Johnson, Rie and Zhang, Tong. Accelerating stochas-
tic gradient descent using predictive variance reduction.
In Advances in Neural Information Processing Systems,
NIPS 2013, pp. 315â€“323, 2013.

Lee, Jason D., Simchowitz, Max, Jordan, Michael I., and
Recht, Benjamin. Gradient descent only converges to
minimizers. In Proceedings of the 29th Conference on
Learning Theory, COLT 2016, New York, USA, June 23-
26, 2016, pp. 1246â€“1257, 2016.

Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid.
A Universal Catalyst
for First-Order Optimization.
In NIPS, 2015. URL http://arxiv.org/pdf/
1506.02186v1.pdf.

Nesterov, Yurii. Accelerating the cubic regularization of
newtonâ€™s method on convex problems. Mathematical
Programming, 112(1):159â€“181, 2008.

Reddi, Sashank J., Hefny, Ahmed, Sra, Suvrit, Poczos,
Barnabas, and Smola, Alex. Stochastic variance re-
duction for nonconvex optimization. ArXiv e-prints,
abs/1603.06160, March 2016.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Min-
imizing ï¬nite sums with the stochastic average gradi-
ent. arXiv preprint arXiv:1309.2388, pp. 1â€“45, 2013.
URL http://arxiv.org/abs/1309.2388. Pre-
liminary version appeared in NIPS 2012.

Shalev-Shwartz, Shai. SDCA without Duality, Regulariza-

tion, and Individual Convexity. In ICML, 2016.

Zhang, Lijun, Mahdavi, Mehrdad, and Jin, Rong. Linear
convergence with condition number independent access
of full gradients. In Advances in Neural Information Pro-
cessing Systems, pp. 980â€“988, 2013.

